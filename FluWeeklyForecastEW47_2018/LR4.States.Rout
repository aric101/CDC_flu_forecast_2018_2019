
R version 3.4.1 (2017-06-30) -- "Single Candle"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-redhat-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> ##########################################################
> ############# Data Loading  ##############################
> ##########################################################
> rm(list=ls())
> week2season <- function(k)c((40:53),(1:39))[k]
> week2season(5)
[1] 44
> 
> #wkID <- (5:8) + (0)
> wkID <- (4:7) + (5) # EW47-2018
> 
> system("ls ./data/*.csv")
./data/ILINetCA.csv
./data/ILINet.csv
./data/ILINetHHS.csv
./data/national1-4wk.forecast.template.csv
./data/national.onset.wk.forecast.template.csv
./data/national.pk.prev.forecast.template.csv
./data/national.pk.wk.forecast.template.csv
./data/wILI_Baseline.csv
> 
> #Region.name <- paste("Region ",1:10 ,sep="")
> Region.name <- 
+ structure(c(2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 
+ 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 
+ 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 36L, 37L, 38L, 39L, 40L, 
+ 41L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 56L, 
+ 35L, 51L, 42L), .Label = c("", "Alabama", "Alaska", "Arizona", 
+ "Arkansas", "California", "Colorado", "Connecticut", "Delaware", 
+ "District of Columbia", "Florida", "Georgia", "Hawaii", "Idaho", 
+ "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", 
+ "Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota", 
+ "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada", "New Hampshire", 
+ "New Jersey", "New Mexico", "New York", "New York City", "North Carolina", 
+ "North Dakota", "Ohio", "Oklahoma", "Oregon", "Pennsylvania", 
+ "Puerto Rico", "REGION", "Rhode Island", "South Carolina", "South Dakota", 
+ "Tennessee", "Texas", "Utah", "Vermont", "Virgin Islands", "Virginia", 
+ "Washington", "West Virginia", "Wisconsin", "Wyoming"), class = "factor")
> Region.name <- as.character(Region.name)
> #Region.name <- Region.name[-c(10, 19, 53)]
> Region.name <- Region.name[-c(10)]
> 
> library(foreach)
> library(doSNOW)
Loading required package: iterators
Loading required package: snow
> #setup parallel backend to use 8 processors
> cl<-makeCluster(4)
> registerDoSNOW(cl)
> #start time
> strt<-Sys.time()
> #loop
> foreach(HHS.id=1:length(Region.name)) %dopar% {
+ #for( HHS.id in 1:length(Region.name)){
+ 
+ #HHS.id <- 1
+ 
+ flu.data <- read.csv("./data/ILINetCA.csv", header=F)[-(1:2),]
+ names(flu.data) <- as.character(read.csv("./data/ILINetCA.csv", header=F, colClasses="character")[2,])
+ flu.data <- flu.data[,-5]
+ flu.data[,5][which(flu.data[,5]=="X")] <- 0
+ names(flu.data)[5] <- "ILI"
+ head(flu.data)
+ #flu.data <- flu.data[-which(flu.data$ILI=="X"),]
+ 
+ flu.data$ILI <- as.numeric(as.character(flu.data$ILI))
+ flu.data$YEAR <- as.numeric(as.character(flu.data$YEAR))
+ flu.data$WEEK <- as.numeric(as.character(flu.data$WEEK))
+ #flu.data$ILI <- signif(flu.data$ILI,2)
+ 
+ flu.data <- flu.data[flu.data$REGION==Region.name[HHS.id],]
+ 
+ flu.data.wk.prev <- flu.data[,c("YEAR","WEEK","ILI")]
+ flu.data.wk.prev$WEEK <- flu.data.wk.prev$WEEK - 39
+ flu.data.wk.prev$YEAR[flu.data.wk.prev$WEEK<=0] <- flu.data.wk.prev$YEAR[flu.data.wk.prev$WEEK<=0] -1
+ flu.data.wk.prev$WEEK[flu.data.wk.prev$WEEK<=0] <- flu.data.wk.prev$WEEK[flu.data.wk.prev$WEEK<=0] + 53
+ tail(flu.data.wk.prev)
+ flu.data.wk.prev <- flu.data.wk.prev[flu.data.wk.prev$WEEK>=(head(wkID,1)-3) & flu.data.wk.prev$WEEK<=tail(wkID+1,1),]
+ ## flu.data.wk.prev <- flu.data.wk.prev[
+ ## (flu.data.wk.prev$YEAR<2016 & flu.data.wk.prev$WEEK>=head(wkID,1) & flu.data.wk.prev$WEEK<=tail(wkID,1)) |
+ ## (flu.data.wk.prev$YEAR==2016 & flu.data.wk.prev$WEEK<=tail(wkID+1,1))
+ ## ,]
+ 
+ tmp.ILI <- tail(flu.data.wk.prev$ILI[flu.data.wk.prev$YEAR==2018], 1)
+ tmp.WEEK <- tail(flu.data.wk.prev$WEEK[flu.data.wk.prev$YEAR==2018], 1)
+ tmp.YEAR <- flu.data.wk.prev$YEAR[flu.data.wk.prev$WEEK==tmp.WEEK & flu.data.wk.prev$ILI <= (tmp.ILI+0.3) & flu.data.wk.prev$ILI >= (tmp.ILI-0.3)]
+ tmp.rows <- sapply(1:length(tmp.YEAR), function(k)which(flu.data.wk.prev$YEAR == tmp.YEAR[k]))
+ tmp.rows <- unlist(tmp.rows)
+ flu.data.wk.prev <- flu.data.wk.prev[tmp.rows,]
+ 
+ 
+ flu.data.yr.prev <- flu.data[,c("YEAR","WEEK","ILI")]
+ flu.data.yr.prev$WEEK <- flu.data.yr.prev$WEEK - 39
+ flu.data.yr.prev$YEAR[flu.data.yr.prev$WEEK<=0] <- flu.data.yr.prev$YEAR[flu.data.yr.prev$WEEK<=0] -1
+ flu.data.yr.prev$WEEK[flu.data.yr.prev$WEEK<=0] <- flu.data.yr.prev$WEEK[flu.data.yr.prev$WEEK<=0] + 53
+ head(flu.data.yr.prev)
+ flu.data.yr.prev <- flu.data.yr.prev[flu.data.yr.prev$YEAR<2018, ]
+ flu.data.yr.prev
+ 
+ flu.data.yr.prev
+ tmp.pk.week <- 
+ lapply(min(flu.data.yr.prev$YEAR) : max(flu.data.yr.prev$YEAR), function(yr){
+   flu.data.yr.prev$WEEK[which(flu.data.yr.prev$YEAR==yr & flu.data.yr.prev$ILI >= -0.1+max(flu.data.yr.prev$ILI[flu.data.yr.prev$YEAR==yr]) & flu.data.yr.prev$ILI <= 0.1+max(flu.data.yr.prev$ILI[flu.data.yr.prev$YEAR==yr]))]
+ })
+ 
+ years <- min(flu.data.yr.prev$YEAR) : max(flu.data.yr.prev$YEAR)
+ flu.data.yr.prev.pk.wk <- vector()
+ for(ii in 1:length(tmp.pk.week)){
+   for(kk in 1:length(tmp.pk.week[[ii]])){
+     flu.data.yr.prev.pk.wk <- rbind(flu.data.yr.prev.pk.wk, c(years[ii], tmp.pk.week[[ii]][kk]))
+   }
+ }
+ 
+ 
+ tmp.pk.prev <- 
+ lapply(min(flu.data.yr.prev$YEAR) : max(flu.data.yr.prev$YEAR), function(yr){
+   flu.data.yr.prev$ILI[which(flu.data.yr.prev$YEAR==yr & flu.data.yr.prev$ILI >= -0.1+max(flu.data.yr.prev$ILI[flu.data.yr.prev$YEAR==yr]) & flu.data.yr.prev$ILI <= 0.1+max(flu.data.yr.prev$ILI[flu.data.yr.prev$YEAR==yr]))]
+ })
+ 
+ years <- min(flu.data.yr.prev$YEAR) : max(flu.data.yr.prev$YEAR)
+ flu.data.yr.prev.pk.prev <- vector()
+ for(ii in 1:length(tmp.pk.prev)){
+   for(kk in 1:length(tmp.pk.prev[[ii]])){
+     flu.data.yr.prev.pk.prev <- rbind(flu.data.yr.prev.pk.prev, c(years[ii], tmp.pk.prev[[ii]][kk]))
+   }
+ }
+ 
+ 
+ 
+ ##########################################################
+ ##########################################################
+ ##########################################################
+ 
+ ################################################################
+ ############# Logistic Regression ##############################
+ ################################################################
+ sim.forecast.ili.LIST <- list()
+ for(j in 1:50){
+ 
+ err.ILI.cv <- -Inf
+ while(sum(is.infinite(err.ILI.cv))>0 | sum(is.na(err.ILI.cv)) >0){
+ 
+ X.input <- data.frame(year=flu.data.wk.prev$YEAR, week=flu.data.wk.prev$WEEK)
+ Y.output <- data.frame(ili=flu.data.wk.prev$ILI)
+ 
+ head(X.input)
+ head(Y.output)
+ 
+ id.random <- which(X.input$year>=1997)
+ #id.random <- which(X.input$year>=2011)
+ id.random <- sample(id.random, length(id.random))
+ #id.random <- sample(1:length(id.random), length(id.random))
+ id.test <- which(X.input$year>=2015)
+ 
+ id.random1 <- id.random[1:floor(length(id.random)*0.8)]
+ id.random2 <- id.random[floor(length(id.random)*0.8):length(id.random)]
+ 
+ scale <- 10 # 1: 130 bins, 10: 1300 bins
+ 
+ ### training data set and Standardization/Rescaling ###
+ #X.data <- X.input[id.random1,] 
+ X.data <- as.data.frame(X.input[id.random1,-1] )
+ X.mean <- sapply(X.data, mean)
+ X.sd <- sapply(X.data, sd)
+ X.min <- sapply(X.data, min)
+ X.max <- sapply(X.data, max)
+ X.data <- sapply(X.data, function(tmp)(tmp-mean(tmp))/sd(tmp)) # Standardization
+ #X.data <- sapply(X.data, function(tmp)(tmp-min(tmp))/(max(tmp)-min(tmp))) # Rescaling
+ ##############################################
+ 
+ ### test data set ###
+ #X.input[id.2011,]$year <- 2020
+ #X.input[id.2011,]$year <- 2011
+ #X.input[id.2011,]$year <- 2012
+ #X.input[id.2011,]$year <- 2015
+ X.test <- t(sapply(1:length(id.test),function(k)(X.input[id.test[k],-1]-X.mean)/X.sd)) # Standardization
+ #X.2011 <- t(sapply(1:length(id.2011),function(k)(X.input[id.2011[k],]-X.min)/(X.max-X.min))) # Rescaling
+ X.test <- (t(X.test))
+ X.test <- cbind(rep(1,nrow(X.test)), X.test)
+ X.test <- matrix(unlist(X.test), nrow(X.test), ncol(X.test))
+ 
+ X.forecast <- tail(X.input[,-1],4)
+ #X.forecast$year <- 2016
+ X.forecast <- wkID
+ X.forecast <- as.matrix(X.forecast)
+ X.forecast <- t(sapply(1:4,function(k)(X.forecast[k,]-X.mean)/X.sd)) # Standardization
+ #X.2011 <- t(sapply(1:length(id.2011),function(k)(X.input[id.2011[k],]-X.min)/(X.max-X.min))) # Rescaling
+ X.forecast <- (t(X.forecast))
+ X.forecast <- cbind(rep(1,nrow(X.forecast)), X.forecast)
+ X.forecast <- matrix(unlist(X.forecast), nrow(X.forecast), ncol(X.forecast))
+ 
+ #####################
+ 
+ ### cross valication data set ###
+ X.cv <- t(sapply(1:length(id.random2),function(k)(X.input[id.random2[k],-1]-X.mean)/X.sd)) # Standardization
+ #X.2011 <- t(sapply(1:length(id.2011),function(k)(X.input[id.2011[k],]-X.min)/(X.max-X.min))) # Rescaling
+ X.cv <- t(X.cv)
+ X.cv <- cbind(rep(1,nrow(X.cv)), X.cv)
+ X.cv <- matrix(unlist(X.cv), nrow(X.cv), ncol(X.cv))
+ #####################
+ 
+ ### initialize paraemters and variables ###
+ Theta <- matrix(rep(0, 1+ncol(X.data)), 1+ncol(X.data), 1)
+ X <- matrix( unlist(cbind(rep(1, nrow(X.data)),X.data)), nrow(X.data), 1+ncol(X.data))
+ 
+ Y.data <- floor(scale*Y.output$ili[id.random1]) == 0
+ Y <- matrix(Y.data*1, nrow=length(id.random1), ncol=1)
+ 
+ m <- nrow(X) # number of examples
+ lambda <- 0.01 # regularization param, under fit if too large, over fit if to small
+ 
+ J <- 0
+ Theta.grad <- rep(0, length(Theta)) # initial parameter Thata
+ ###########################################
+ 
+ ### hypothesis function ###
+ hypo <- function(Theta){
+   1/(1+exp(-1*(X %*% Theta))) # hypothesis function
+ }
+ #h.prob <- hypo(Theta) 
+ ###########################
+ 
+ ### cost function ###
+ cost.fn <- function(Theta){
+   h.prob <- hypo(Theta) # hypothesis function
+   J <- sum(Y * -log(h.prob) + (1-Y) * (-log(1-h.prob)))/m # cost function
+   J <- J + sum(lambda/(2*m)*(Theta[-1])^2) # regularization term
+   return(J)
+ }
+ #####################
+ 
+ ### gradient function ###
+ grad.fn <- function(Theta){
+   h.prob <- hypo(Theta) # hypothesis function
+   Theta.grad <- apply(X, 2, function(xj)sum((h.prob - Y) * xj)/m) # dCost/dTheta
+   Theta.grad <- Theta.grad + c(0, lambda/m * Theta[-1]) # regularization term
+   return(Theta.grad)
+ }
+ ########################
+ 
+ ## forecast function ###
+ forecast <- function(Theta, X){
+   1/(1+exp(-1*(X %*% Theta))) 
+ }
+ ########################
+ 
+ ## forecast Error function ###
+ cost.ERR <- function(Theta, X, Y){
+   h.prob <- forecast(Theta, X) # hypothesis function
+   J <- sum(Y * -log(h.prob) + (1-Y) * (-log(1-h.prob)))/nrow(X) # cost function
+   return(J)
+ }
+ ########################
+ 
+ ################################################################
+ ################################################################
+ ################################################################
+ ILI.Theta <- list()
+ err.ILI.train <- vector()
+ err.ILI.cv <- vector()
+ 
+ for( i in (-5:11)){# big loop
+ lambda <- (2^(i/1))/100
+ ##############################################
+ ###### ILI prediction using test-dataset ######
+ ##############################################
+ Theta.all <- vector()
+ system.time(
+ for(kk in 0:130){
+   Y.data <- floor(scale*Y.output$ili[id.random1]) == kk
+   Y <- matrix(Y.data*1, nrow=length(id.random1), ncol=1)
+   Theta.optm <- optim(par=Theta, fn=cost.fn, gr=grad.fn, method="BFGS")
+   #Theta.optm <- optim(par=Theta, fn=cost.fn, method="Nelder-Mead")
+   Theta.optm <- Theta.optm$par
+   Theta.all <- cbind(Theta.all, Theta.optm)
+ }
+ )
+ 
+ ILI.Theta <- c(ILI.Theta, list(Theta.all))
+ 
+ err.ILI.tmp <- vector()
+ for(kk in 0:130){
+   Y.data <- floor(scale*Y.output$ili[id.random1]) == kk
+   Y <- matrix(Y.data*1, nrow=length(id.random1), ncol=1)
+   tmp <- cost.ERR(Theta.all[,(kk+1)], X, Y)
+   err.ILI.tmp <- c(err.ILI.tmp, tmp)
+ }
+ err.ILI.train <- c(err.ILI.train, sum(err.ILI.tmp))
+ 
+ err.ILI.cv.tmp <- vector()
+ for(kk in 0:130){
+   Y.data.cv <- floor(scale*Y.output$ili[id.random2]) == kk
+   Y.cv <- matrix(Y.data.cv*1, nrow=length(id.random2), ncol=1)
+   tmp <- cost.ERR(Theta.all[,(kk+1)], X.cv, Y.cv)
+   err.ILI.cv.tmp <- c(err.ILI.cv.tmp, tmp)
+ }
+ err.ILI.cv <- c(err.ILI.cv, sum(err.ILI.cv.tmp))
+ 
+ ##############################################
+ ##############################################
+ ##############################################
+ 
+ }# big loop
+ 
+ } # end of while()
+ 
+ D <- length(err.ILI.train) - 1
+ total.cost <- c(err.ILI.train, err.ILI.cv)
+ plot(2^(0:D)/100, err.ILI.cv, type="l", ylim=c(min(total.cost), max(total.cost)), col="green", xlab="lambda", ylab="cost")
+ lines(2^(0:D)/100, err.ILI.train, type="l", col="red")
+ 
+ lambda.ili.id <- which((err.ILI.cv) == min(err.ILI.cv))[1]
+ 
+ #lambda.tf.id <- which((err.TF.cv+err.TT.cv) == min(err.TF.cv+err.TT.cv))
+ #lambda.tt.id <- which((err.TF.cv+err.TT.cv) == min(err.TF.cv+err.TT.cv))
+ 
+ ##############################################
+ ##############################################
+ ##############################################
+ ### forecasts of test data 2014/2015, 2015/2016
+ sim.test.ili <- vector()
+ for(k in 1:length(id.test)){
+   tmp <- c(forecast(ILI.Theta[[lambda.ili.id]], X.test[k,]))
+   sim.test.ili <- cbind(sim.test.ili, tmp/sum(tmp))
+ }
+ 
+ ## ILI
+ sim.test.ili.avg <- apply(sim.test.ili, 1, mean)
+ 
+ ili.mean.lr <- sum(sim.test.ili.avg*c(0:130))/scale
+ ili.loglik.lr <- sum(log(sim.test.ili.avg[1+floor(scale*Y.output$ili[id.test])]))
+ 
+ ili.mean.true <- mean(Y.output$ili[id.test])
+ 
+ plot.ILI <- function(){
+ plot((0:130)/10, sim.test.ili[,1], type="l", main=paste("TEST:",Region.name[HHS.id]," flu seasons 2015/2016, 2016/2018"), xlab="%ILI", ylab="density", ylim=c(0,0.1))
+ abline(v=(scale*Y.output$ili[id.test])/10, col="grey")
+ sapply(1:ncol(sim.test.ili),function(kk)lines((0:130)/10, sim.test.ili[,kk], type="l"))
+ }
+ 
+ ### forecasts of test data 2016/2018
+ sim.forecast.ili <- vector()
+ for(k in 1:nrow(X.forecast)){
+   tmp <- c(forecast(ILI.Theta[[lambda.ili.id]], X.forecast[k,]))
+   sim.forecast.ili <- cbind(sim.forecast.ili, tmp/sum(tmp))
+ }
+ 
+ plot.ILI.forecast <- function(){
+ plot((0:130)/10, sim.forecast.ili[,1], type="l", main=paste("FORECAST:", Region.name[HHS.id],"flu seasons 2018/2019 1-4 weeks"), xlab="%ILI", ylab="density", ylim=c(0,0.1))
+ sapply(1:ncol(sim.forecast.ili),function(kk)lines((0:130)/10, sim.forecast.ili[,kk], type="l"))
+ }
+ 
+ ##############################
+ ###### plot results ##########
+ ##############################
+ 
+ png(paste("./plots/Region",HHS.id,"1-4 week forecast.png",sep=""),width=6,height=8,units="in",res=150)
+ 
+ par(mfrow = c(3,1))
+ plot(2^(0:D)/100, err.ILI.cv, type="l", col="green", xlab="lambda", ylab="cost", ylim=c(min(c(err.ILI.cv, err.ILI.train)), max(c(err.ILI.cv, err.ILI.train))))
+ lines(2^(0:D)/100, err.ILI.train, type="l", col="red")
+ legend("bottomright", c("training Error","cross validation Error"), lty=1, col=c("red", "green"))
+ plot.ILI()
+ plot.ILI.forecast()
+ 
+ dev.off()
+ ##############################
+ ##############################
+ ##############################
+ sim.forecast.ili.LIST <- c(sim.forecast.ili.LIST, list(sim.forecast.ili))
+ } # external for-loop
+ 
+ tmp <- sim.forecast.ili.LIST[[1]]
+ for(i in 2:length(sim.forecast.ili.LIST)){
+   tmp <- tmp + sim.forecast.ili.LIST[[i]]
+ }
+ 
+ sim.forecast.ili <- tmp/length(sim.forecast.ili.LIST)
+ avg.ili <- sapply(1:length(sim.forecast.ili.LIST), function(k)apply(sim.forecast.ili.LIST[[k]], 2, function(x)sum(x*(0:130))))
+ 
+ data.smooth <- 
+ sapply(1:ncol(sim.forecast.ili), function(k){
+ tmp.mean <- sum(c(0:130)/10*sim.forecast.ili[,k])
+ #tmp.sd <- sd(sample(x=(0:130)/10, 10000, replace=T, prob=sim.forecast.ili[,k]))/2
+ tmp.sd <- sd(sample(x=(0:130)/10, 10000, replace=T, prob=sim.forecast.ili[,k]))/1
+ if(tmp.sd==0)return(sim.forecast.ili[,k])
+ tmp <- dnorm(x=(0:130)/10, mean=tmp.mean, sd=tmp.sd)
+ tmp/sum(tmp)
+ })
+ 
+ 
+ national4wks <- read.csv("national1-4wk.forecast.template.csv")
+ head(national4wks)
+ national4wks$Location <- paste(Region.name[HHS.id])
+ 
+ nrow(national4wks)
+ 
+ national4wks$Value[1] <- signif(sum((0:130)*sim.forecast.ili[,1])/10, 2)
+ national4wks$Value[1+132] <- signif(sum((0:130)*sim.forecast.ili[,2])/10, 2)
+ national4wks$Value[1+132+132] <- signif(sum((0:130)*sim.forecast.ili[,3])/10, 2)
+ national4wks$Value[1+132+132+132] <- signif(sum((0:130)*sim.forecast.ili[,4])/10, 2)
+ 
+ national4wks$Value[(2):(2+130)] <- signif(sim.forecast.ili[,1],2)
+ national4wks$Value[(2+130+2):(2+130+2+130)] <- signif(sim.forecast.ili[,2],2)
+ national4wks$Value[(2+130+2+130+2):(2+130+2+130+2+130)] <- signif(sim.forecast.ili[,3],2)
+ national4wks$Value[(2+130+2+130+2+130+2):(2+130+2+130+2+130+2+130)] <- signif(sim.forecast.ili[,4],2)
+ 
+ write.csv(national4wks, paste("./output/State",HHS.id,"4wks.csv",sep=""), row.names=F)
+ 
+ for(k in 1:ncol(sim.forecast.ili)){
+   sim.forecast.ili[,k] <- data.smooth[,k]
+ }
+ 
+ national4wks <- read.csv("national1-4wk.forecast.template.csv")
+ head(national4wks)
+ national4wks$Location <- paste(Region.name[HHS.id])
+ 
+ nrow(national4wks)
+ 
+ national4wks$Value[1] <- signif(sum((0:130)*sim.forecast.ili[,1])/10, 2)
+ national4wks$Value[1+132] <- signif(sum((0:130)*sim.forecast.ili[,2])/10, 2)
+ national4wks$Value[1+132+132] <- signif(sum((0:130)*sim.forecast.ili[,3])/10, 2)
+ national4wks$Value[1+132+132+132] <- signif(sum((0:130)*sim.forecast.ili[,4])/10, 2)
+ 
+ national4wks$Value[(2):(2+130)] <- signif(sim.forecast.ili[,1],2)
+ national4wks$Value[(2+130+2):(2+130+2+130)] <- signif(sim.forecast.ili[,2],2)
+ national4wks$Value[(2+130+2+130+2):(2+130+2+130+2+130)] <- signif(sim.forecast.ili[,3],2)
+ national4wks$Value[(2+130+2+130+2+130+2):(2+130+2+130+2+130+2+130)] <- signif(sim.forecast.ili[,4],2)
+ 
+ write.csv(national4wks, paste("./output/State",HHS.id,"4wksSMOOTH.csv",sep=""), row.names=F)
+ 
+ 
+ flu.data.wk.prev$ILI[flu.data.wk.prev$YEAR==2018]
+ 
+ projection <- apply(avg.ili,2, function(x)c(flu.data.wk.prev$ILI[flu.data.wk.prev$YEAR==2018], x/10))
+ png(paste("./plots/forecastState",HHS.id,".png",sep=""))
+ plot(week2season(head(wkID,1)-3):week2season(tail(wkID,1)), projection[,1], lty=2,type="l", col="grey", xlab="wk", ylab="ILI%", ylim=c(0.2,3), , main=paste("State: ", Region.name[HHS.id], " 1-4 wk ahead",sep=""))
+ sapply(1:ncol(projection),function(k)lines(week2season(head(wkID,1)-3):week2season(tail(wkID,1)),projection[,k], lty=2,type="l", col="grey"))
+ lines(week2season(head(wkID,1)-3):(week2season(head(wkID,1))-1), flu.data.wk.prev$ILI[flu.data.wk.prev$YEAR==2018], lwd=3)
+ lines((week2season(head(wkID,1))-1):week2season(tail(wkID,1)), c(tail(flu.data.wk.prev$ILI,1), apply(avg.ili, 1, mean)/10), lty=2, col="red",lwd=3)
+ dev.off()
+ 
+ apply(avg.ili, 1, mean)
+ 
+ }
[[1]]
[1] 39.71313 40.52442 41.49540 42.49442

[[2]]
[1] 25.44971 25.33151 25.22494 25.13281

[[3]]
[1] 26.34780 26.55175 26.72690 26.87042

[[4]]
[1] 25.01898 25.22091 25.39022 25.52626

[[5]]
[1] 25.79032 26.23554 26.75986 27.35451

[[6]]
[1] 29.20473 29.21954 29.23089 29.23930

[[7]]
[1] 24.99671 25.30396 25.61780 25.90990

[[8]]
[1] 2.163826 2.213114 2.268897 2.342366

[[9]]
[1] 18.36580 18.24499 18.13515 18.04117

[[10]]
[1] 40.94377 41.26899 41.54612 41.77176

[[11]]
[1] 20.58859 20.72165 20.86729 21.02148

[[12]]
[1] 7.381766 7.382838 7.373237 7.358497

[[13]]
[1] 21.28981 21.87628 22.53603 23.22159

[[14]]
[1] 32.19416 33.02748 33.91847 34.76660

[[15]]
[1]  7.840241  8.691811  9.704195 10.840064

[[16]]
[1] 22.05461 22.58657 23.13053 23.68415

[[17]]
[1] 25.00002 25.00002 25.00002 25.00002

[[18]]
[1] 48.06724 48.52524 48.99063 49.41382

[[19]]
[1] 8.988133 8.992879 8.996900 9.000313

[[20]]
[1] 17.43259 17.89572 18.41963 18.99480

[[21]]
[1] 15.81053 15.94891 16.12655 16.42474

[[22]]
[1] 17.88089 18.23379 18.67431 19.16282

[[23]]
[1] 22.29873 22.40693 22.49948 22.57505

[[24]]
[1] 32.43801 32.50611 32.57512 32.64475

[[25]]
[1] 19.87145 20.21714 20.57107 20.93171

[[26]]
[1]  9.85541 10.04122 10.20052 10.33085

[[27]]
[1] 31.43954 31.57370 31.74841 31.96122

[[28]]
[1] 13.01070 13.30415 13.55216 13.75275

[[29]]
[1] 2.624667 2.795915 3.056437 3.468255

[[30]]
[1] 34.27711 34.51355 34.76135 35.00844

[[31]]
[1] 19.36870 20.09981 20.93826 21.83732

[[32]]
[1] 18.04286 18.47935 19.07425 19.87297

[[33]]
[1] 25.70323 26.17786 26.64987 27.11075

[[34]]
[1] 8.265857 8.600036 8.994028 9.425943

[[35]]
[1] 9.085361 9.308792 9.579242 9.919325

[[36]]
[1] 31.08957 31.10633 31.12632 31.14824

[[37]]
[1] 12.91566 13.19157 13.48111 13.78117

[[38]]
[1] 21.66935 22.38979 23.15912 23.94784

[[39]]
[1] 6.067769 6.322608 6.628956 7.002856

[[40]]
[1] 26.17351 26.23221 26.28583 26.33184

[[41]]
[1]  9.598278  9.809193 10.062227 10.366266

[[42]]
[1] 20.54820 20.99413 21.49349 21.97511

[[43]]
[1] 32.75264 32.86060 32.95168 33.02523

[[44]]
[1] 30.71062 31.17381 31.65601 32.12133

[[45]]
[1] 5.995428 6.037633 6.071981 6.100035

[[46]]
[1] 25.60293 25.73702 25.85129 25.94435

[[47]]
[1]  9.597204 10.082743 10.719261 11.490832

[[48]]
[1] 16.90465 17.11309 17.32474 17.53939

[[49]]
[1] 14.28058 14.28825 14.29323 14.29625

[[50]]
[1] 7.937760 8.089362 8.271107 8.513542

[[51]]
[1] 26.44941 26.56608 26.66489 26.74494

[[52]]
[1]  9.893630  9.934700  9.976793 10.019901

[[53]]
[1] 89.69996 89.69999 89.69999 89.69999

> 
> print(Sys.time()-strt)
Time difference of 55.10173 mins
> stopCluster(cl)
> 
> 
> proc.time()
    user   system  elapsed 
   1.117    0.052 3307.100 
