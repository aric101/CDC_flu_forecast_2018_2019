
R version 3.4.1 (2017-06-30) -- "Single Candle"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-redhat-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> ##########################################################
> ############# Data Loading  ##############################
> ##########################################################
> rm(list=ls())
> week2season <- function(k)c((40:53),(1:39))[k]
> week2season(5)
[1] 44
> 
> #wkID <- (5:8) + (0)
> wkID <- (4:7) + (3) # EW45-2018
> 
> system("ls ./data/*.csv")
./data/ILINetCA.csv
./data/ILINet.csv
./data/ILINetHHS.csv
./data/national1-4wk.forecast.template.csv
./data/national.onset.wk.forecast.template.csv
./data/national.pk.prev.forecast.template.csv
./data/national.pk.wk.forecast.template.csv
./data/wILI_Baseline.csv
> 
> #Region.name <- paste("Region ",1:10 ,sep="")
> Region.name <- 
+ structure(c(2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 
+ 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 
+ 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 36L, 37L, 38L, 39L, 40L, 
+ 41L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 56L, 
+ 35L, 51L, 42L), .Label = c("", "Alabama", "Alaska", "Arizona", 
+ "Arkansas", "California", "Colorado", "Connecticut", "Delaware", 
+ "District of Columbia", "Florida", "Georgia", "Hawaii", "Idaho", 
+ "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", 
+ "Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota", 
+ "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada", "New Hampshire", 
+ "New Jersey", "New Mexico", "New York", "New York City", "North Carolina", 
+ "North Dakota", "Ohio", "Oklahoma", "Oregon", "Pennsylvania", 
+ "Puerto Rico", "REGION", "Rhode Island", "South Carolina", "South Dakota", 
+ "Tennessee", "Texas", "Utah", "Vermont", "Virgin Islands", "Virginia", 
+ "Washington", "West Virginia", "Wisconsin", "Wyoming"), class = "factor")
> Region.name <- as.character(Region.name)
> #Region.name <- Region.name[-c(10, 19, 53)]
> Region.name <- Region.name[-c(10)]
> 
> library(foreach)
> library(doSNOW)
Loading required package: iterators
Loading required package: snow
> #setup parallel backend to use 8 processors
> cl<-makeCluster(4)
> registerDoSNOW(cl)
> #start time
> strt<-Sys.time()
> #loop
> foreach(HHS.id=1:length(Region.name)) %dopar% {
+ #for( HHS.id in 1:length(Region.name)){
+ 
+ #HHS.id <- 1
+ 
+ flu.data <- read.csv("./data/ILINetCA.csv", header=F)[-(1:2),]
+ names(flu.data) <- as.character(read.csv("./data/ILINetCA.csv", header=F, colClasses="character")[2,])
+ flu.data <- flu.data[,-5]
+ flu.data[,5][which(flu.data[,5]=="X")] <- 0
+ names(flu.data)[5] <- "ILI"
+ head(flu.data)
+ #flu.data <- flu.data[-which(flu.data$ILI=="X"),]
+ 
+ flu.data$ILI <- as.numeric(as.character(flu.data$ILI))
+ flu.data$YEAR <- as.numeric(as.character(flu.data$YEAR))
+ flu.data$WEEK <- as.numeric(as.character(flu.data$WEEK))
+ #flu.data$ILI <- signif(flu.data$ILI,2)
+ 
+ flu.data <- flu.data[flu.data$REGION==Region.name[HHS.id],]
+ 
+ flu.data.wk.prev <- flu.data[,c("YEAR","WEEK","ILI")]
+ flu.data.wk.prev$WEEK <- flu.data.wk.prev$WEEK - 39
+ flu.data.wk.prev$YEAR[flu.data.wk.prev$WEEK<=0] <- flu.data.wk.prev$YEAR[flu.data.wk.prev$WEEK<=0] -1
+ flu.data.wk.prev$WEEK[flu.data.wk.prev$WEEK<=0] <- flu.data.wk.prev$WEEK[flu.data.wk.prev$WEEK<=0] + 53
+ tail(flu.data.wk.prev)
+ flu.data.wk.prev <- flu.data.wk.prev[flu.data.wk.prev$WEEK>=(head(wkID,1)-3) & flu.data.wk.prev$WEEK<=tail(wkID+1,1),]
+ ## flu.data.wk.prev <- flu.data.wk.prev[
+ ## (flu.data.wk.prev$YEAR<2016 & flu.data.wk.prev$WEEK>=head(wkID,1) & flu.data.wk.prev$WEEK<=tail(wkID,1)) |
+ ## (flu.data.wk.prev$YEAR==2016 & flu.data.wk.prev$WEEK<=tail(wkID+1,1))
+ ## ,]
+ 
+ tmp.ILI <- tail(flu.data.wk.prev$ILI[flu.data.wk.prev$YEAR==2018], 1)
+ tmp.WEEK <- tail(flu.data.wk.prev$WEEK[flu.data.wk.prev$YEAR==2018], 1)
+ tmp.YEAR <- flu.data.wk.prev$YEAR[flu.data.wk.prev$WEEK==tmp.WEEK & flu.data.wk.prev$ILI <= (tmp.ILI+0.3) & flu.data.wk.prev$ILI >= (tmp.ILI-0.3)]
+ tmp.rows <- sapply(1:length(tmp.YEAR), function(k)which(flu.data.wk.prev$YEAR == tmp.YEAR[k]))
+ tmp.rows <- unlist(tmp.rows)
+ flu.data.wk.prev <- flu.data.wk.prev[tmp.rows,]
+ 
+ 
+ flu.data.yr.prev <- flu.data[,c("YEAR","WEEK","ILI")]
+ flu.data.yr.prev$WEEK <- flu.data.yr.prev$WEEK - 39
+ flu.data.yr.prev$YEAR[flu.data.yr.prev$WEEK<=0] <- flu.data.yr.prev$YEAR[flu.data.yr.prev$WEEK<=0] -1
+ flu.data.yr.prev$WEEK[flu.data.yr.prev$WEEK<=0] <- flu.data.yr.prev$WEEK[flu.data.yr.prev$WEEK<=0] + 53
+ head(flu.data.yr.prev)
+ flu.data.yr.prev <- flu.data.yr.prev[flu.data.yr.prev$YEAR<2018, ]
+ flu.data.yr.prev
+ 
+ flu.data.yr.prev
+ tmp.pk.week <- 
+ lapply(min(flu.data.yr.prev$YEAR) : max(flu.data.yr.prev$YEAR), function(yr){
+   flu.data.yr.prev$WEEK[which(flu.data.yr.prev$YEAR==yr & flu.data.yr.prev$ILI >= -0.1+max(flu.data.yr.prev$ILI[flu.data.yr.prev$YEAR==yr]) & flu.data.yr.prev$ILI <= 0.1+max(flu.data.yr.prev$ILI[flu.data.yr.prev$YEAR==yr]))]
+ })
+ 
+ years <- min(flu.data.yr.prev$YEAR) : max(flu.data.yr.prev$YEAR)
+ flu.data.yr.prev.pk.wk <- vector()
+ for(ii in 1:length(tmp.pk.week)){
+   for(kk in 1:length(tmp.pk.week[[ii]])){
+     flu.data.yr.prev.pk.wk <- rbind(flu.data.yr.prev.pk.wk, c(years[ii], tmp.pk.week[[ii]][kk]))
+   }
+ }
+ 
+ 
+ tmp.pk.prev <- 
+ lapply(min(flu.data.yr.prev$YEAR) : max(flu.data.yr.prev$YEAR), function(yr){
+   flu.data.yr.prev$ILI[which(flu.data.yr.prev$YEAR==yr & flu.data.yr.prev$ILI >= -0.1+max(flu.data.yr.prev$ILI[flu.data.yr.prev$YEAR==yr]) & flu.data.yr.prev$ILI <= 0.1+max(flu.data.yr.prev$ILI[flu.data.yr.prev$YEAR==yr]))]
+ })
+ 
+ years <- min(flu.data.yr.prev$YEAR) : max(flu.data.yr.prev$YEAR)
+ flu.data.yr.prev.pk.prev <- vector()
+ for(ii in 1:length(tmp.pk.prev)){
+   for(kk in 1:length(tmp.pk.prev[[ii]])){
+     flu.data.yr.prev.pk.prev <- rbind(flu.data.yr.prev.pk.prev, c(years[ii], tmp.pk.prev[[ii]][kk]))
+   }
+ }
+ 
+ 
+ 
+ ##########################################################
+ ##########################################################
+ ##########################################################
+ 
+ ################################################################
+ ############# Logistic Regression ##############################
+ ################################################################
+ sim.forecast.ili.LIST <- list()
+ for(j in 1:50){
+ 
+ err.ILI.cv <- -Inf
+ while(sum(is.infinite(err.ILI.cv))>0 | sum(is.na(err.ILI.cv)) >0){
+ 
+ X.input <- data.frame(year=flu.data.wk.prev$YEAR, week=flu.data.wk.prev$WEEK)
+ Y.output <- data.frame(ili=flu.data.wk.prev$ILI)
+ 
+ head(X.input)
+ head(Y.output)
+ 
+ id.random <- which(X.input$year>=1997)
+ #id.random <- which(X.input$year>=2011)
+ id.random <- sample(id.random, length(id.random))
+ #id.random <- sample(1:length(id.random), length(id.random))
+ id.test <- which(X.input$year>=2015)
+ 
+ id.random1 <- id.random[1:floor(length(id.random)*0.8)]
+ id.random2 <- id.random[floor(length(id.random)*0.8):length(id.random)]
+ 
+ scale <- 10 # 1: 130 bins, 10: 1300 bins
+ 
+ ### training data set and Standardization/Rescaling ###
+ #X.data <- X.input[id.random1,] 
+ X.data <- as.data.frame(X.input[id.random1,-1] )
+ X.mean <- sapply(X.data, mean)
+ X.sd <- sapply(X.data, sd)
+ X.min <- sapply(X.data, min)
+ X.max <- sapply(X.data, max)
+ X.data <- sapply(X.data, function(tmp)(tmp-mean(tmp))/sd(tmp)) # Standardization
+ #X.data <- sapply(X.data, function(tmp)(tmp-min(tmp))/(max(tmp)-min(tmp))) # Rescaling
+ ##############################################
+ 
+ ### test data set ###
+ #X.input[id.2011,]$year <- 2020
+ #X.input[id.2011,]$year <- 2011
+ #X.input[id.2011,]$year <- 2012
+ #X.input[id.2011,]$year <- 2015
+ X.test <- t(sapply(1:length(id.test),function(k)(X.input[id.test[k],-1]-X.mean)/X.sd)) # Standardization
+ #X.2011 <- t(sapply(1:length(id.2011),function(k)(X.input[id.2011[k],]-X.min)/(X.max-X.min))) # Rescaling
+ X.test <- (t(X.test))
+ X.test <- cbind(rep(1,nrow(X.test)), X.test)
+ X.test <- matrix(unlist(X.test), nrow(X.test), ncol(X.test))
+ 
+ X.forecast <- tail(X.input[,-1],4)
+ #X.forecast$year <- 2016
+ X.forecast <- wkID
+ X.forecast <- as.matrix(X.forecast)
+ X.forecast <- t(sapply(1:4,function(k)(X.forecast[k,]-X.mean)/X.sd)) # Standardization
+ #X.2011 <- t(sapply(1:length(id.2011),function(k)(X.input[id.2011[k],]-X.min)/(X.max-X.min))) # Rescaling
+ X.forecast <- (t(X.forecast))
+ X.forecast <- cbind(rep(1,nrow(X.forecast)), X.forecast)
+ X.forecast <- matrix(unlist(X.forecast), nrow(X.forecast), ncol(X.forecast))
+ 
+ #####################
+ 
+ ### cross valication data set ###
+ X.cv <- t(sapply(1:length(id.random2),function(k)(X.input[id.random2[k],-1]-X.mean)/X.sd)) # Standardization
+ #X.2011 <- t(sapply(1:length(id.2011),function(k)(X.input[id.2011[k],]-X.min)/(X.max-X.min))) # Rescaling
+ X.cv <- t(X.cv)
+ X.cv <- cbind(rep(1,nrow(X.cv)), X.cv)
+ X.cv <- matrix(unlist(X.cv), nrow(X.cv), ncol(X.cv))
+ #####################
+ 
+ ### initialize paraemters and variables ###
+ Theta <- matrix(rep(0, 1+ncol(X.data)), 1+ncol(X.data), 1)
+ X <- matrix( unlist(cbind(rep(1, nrow(X.data)),X.data)), nrow(X.data), 1+ncol(X.data))
+ 
+ Y.data <- floor(scale*Y.output$ili[id.random1]) == 0
+ Y <- matrix(Y.data*1, nrow=length(id.random1), ncol=1)
+ 
+ m <- nrow(X) # number of examples
+ lambda <- 0.01 # regularization param, under fit if too large, over fit if to small
+ 
+ J <- 0
+ Theta.grad <- rep(0, length(Theta)) # initial parameter Thata
+ ###########################################
+ 
+ ### hypothesis function ###
+ hypo <- function(Theta){
+   1/(1+exp(-1*(X %*% Theta))) # hypothesis function
+ }
+ #h.prob <- hypo(Theta) 
+ ###########################
+ 
+ ### cost function ###
+ cost.fn <- function(Theta){
+   h.prob <- hypo(Theta) # hypothesis function
+   J <- sum(Y * -log(h.prob) + (1-Y) * (-log(1-h.prob)))/m # cost function
+   J <- J + sum(lambda/(2*m)*(Theta[-1])^2) # regularization term
+   return(J)
+ }
+ #####################
+ 
+ ### gradient function ###
+ grad.fn <- function(Theta){
+   h.prob <- hypo(Theta) # hypothesis function
+   Theta.grad <- apply(X, 2, function(xj)sum((h.prob - Y) * xj)/m) # dCost/dTheta
+   Theta.grad <- Theta.grad + c(0, lambda/m * Theta[-1]) # regularization term
+   return(Theta.grad)
+ }
+ ########################
+ 
+ ## forecast function ###
+ forecast <- function(Theta, X){
+   1/(1+exp(-1*(X %*% Theta))) 
+ }
+ ########################
+ 
+ ## forecast Error function ###
+ cost.ERR <- function(Theta, X, Y){
+   h.prob <- forecast(Theta, X) # hypothesis function
+   J <- sum(Y * -log(h.prob) + (1-Y) * (-log(1-h.prob)))/nrow(X) # cost function
+   return(J)
+ }
+ ########################
+ 
+ ################################################################
+ ################################################################
+ ################################################################
+ ILI.Theta <- list()
+ err.ILI.train <- vector()
+ err.ILI.cv <- vector()
+ 
+ for( i in (-5:11)){# big loop
+ lambda <- (2^(i/1))/100
+ ##############################################
+ ###### ILI prediction using test-dataset ######
+ ##############################################
+ Theta.all <- vector()
+ system.time(
+ for(kk in 0:130){
+   Y.data <- floor(scale*Y.output$ili[id.random1]) == kk
+   Y <- matrix(Y.data*1, nrow=length(id.random1), ncol=1)
+   Theta.optm <- optim(par=Theta, fn=cost.fn, gr=grad.fn, method="BFGS")
+   #Theta.optm <- optim(par=Theta, fn=cost.fn, method="Nelder-Mead")
+   Theta.optm <- Theta.optm$par
+   Theta.all <- cbind(Theta.all, Theta.optm)
+ }
+ )
+ 
+ ILI.Theta <- c(ILI.Theta, list(Theta.all))
+ 
+ err.ILI.tmp <- vector()
+ for(kk in 0:130){
+   Y.data <- floor(scale*Y.output$ili[id.random1]) == kk
+   Y <- matrix(Y.data*1, nrow=length(id.random1), ncol=1)
+   tmp <- cost.ERR(Theta.all[,(kk+1)], X, Y)
+   err.ILI.tmp <- c(err.ILI.tmp, tmp)
+ }
+ err.ILI.train <- c(err.ILI.train, sum(err.ILI.tmp))
+ 
+ err.ILI.cv.tmp <- vector()
+ for(kk in 0:130){
+   Y.data.cv <- floor(scale*Y.output$ili[id.random2]) == kk
+   Y.cv <- matrix(Y.data.cv*1, nrow=length(id.random2), ncol=1)
+   tmp <- cost.ERR(Theta.all[,(kk+1)], X.cv, Y.cv)
+   err.ILI.cv.tmp <- c(err.ILI.cv.tmp, tmp)
+ }
+ err.ILI.cv <- c(err.ILI.cv, sum(err.ILI.cv.tmp))
+ 
+ ##############################################
+ ##############################################
+ ##############################################
+ 
+ }# big loop
+ 
+ } # end of while()
+ 
+ D <- length(err.ILI.train) - 1
+ total.cost <- c(err.ILI.train, err.ILI.cv)
+ plot(2^(0:D)/100, err.ILI.cv, type="l", ylim=c(min(total.cost), max(total.cost)), col="green", xlab="lambda", ylab="cost")
+ lines(2^(0:D)/100, err.ILI.train, type="l", col="red")
+ 
+ lambda.ili.id <- which((err.ILI.cv) == min(err.ILI.cv))[1]
+ 
+ #lambda.tf.id <- which((err.TF.cv+err.TT.cv) == min(err.TF.cv+err.TT.cv))
+ #lambda.tt.id <- which((err.TF.cv+err.TT.cv) == min(err.TF.cv+err.TT.cv))
+ 
+ ##############################################
+ ##############################################
+ ##############################################
+ ### forecasts of test data 2014/2015, 2015/2016
+ sim.test.ili <- vector()
+ for(k in 1:length(id.test)){
+   tmp <- c(forecast(ILI.Theta[[lambda.ili.id]], X.test[k,]))
+   sim.test.ili <- cbind(sim.test.ili, tmp/sum(tmp))
+ }
+ 
+ ## ILI
+ sim.test.ili.avg <- apply(sim.test.ili, 1, mean)
+ 
+ ili.mean.lr <- sum(sim.test.ili.avg*c(0:130))/scale
+ ili.loglik.lr <- sum(log(sim.test.ili.avg[1+floor(scale*Y.output$ili[id.test])]))
+ 
+ ili.mean.true <- mean(Y.output$ili[id.test])
+ 
+ plot.ILI <- function(){
+ plot((0:130)/10, sim.test.ili[,1], type="l", main=paste("TEST:",Region.name[HHS.id]," flu seasons 2015/2016, 2016/2018"), xlab="%ILI", ylab="density", ylim=c(0,0.1))
+ abline(v=(scale*Y.output$ili[id.test])/10, col="grey")
+ sapply(1:ncol(sim.test.ili),function(kk)lines((0:130)/10, sim.test.ili[,kk], type="l"))
+ }
+ 
+ ### forecasts of test data 2016/2018
+ sim.forecast.ili <- vector()
+ for(k in 1:nrow(X.forecast)){
+   tmp <- c(forecast(ILI.Theta[[lambda.ili.id]], X.forecast[k,]))
+   sim.forecast.ili <- cbind(sim.forecast.ili, tmp/sum(tmp))
+ }
+ 
+ plot.ILI.forecast <- function(){
+ plot((0:130)/10, sim.forecast.ili[,1], type="l", main=paste("FORECAST:", Region.name[HHS.id],"flu seasons 2018/2019 1-4 weeks"), xlab="%ILI", ylab="density", ylim=c(0,0.1))
+ sapply(1:ncol(sim.forecast.ili),function(kk)lines((0:130)/10, sim.forecast.ili[,kk], type="l"))
+ }
+ 
+ ##############################
+ ###### plot results ##########
+ ##############################
+ 
+ png(paste("./plots/Region",HHS.id,"1-4 week forecast.png",sep=""),width=6,height=8,units="in",res=150)
+ 
+ par(mfrow = c(3,1))
+ plot(2^(0:D)/100, err.ILI.cv, type="l", col="green", xlab="lambda", ylab="cost", ylim=c(min(c(err.ILI.cv, err.ILI.train)), max(c(err.ILI.cv, err.ILI.train))))
+ lines(2^(0:D)/100, err.ILI.train, type="l", col="red")
+ legend("bottomright", c("training Error","cross validation Error"), lty=1, col=c("red", "green"))
+ plot.ILI()
+ plot.ILI.forecast()
+ 
+ dev.off()
+ ##############################
+ ##############################
+ ##############################
+ sim.forecast.ili.LIST <- c(sim.forecast.ili.LIST, list(sim.forecast.ili))
+ } # external for-loop
+ 
+ tmp <- sim.forecast.ili.LIST[[1]]
+ for(i in 2:length(sim.forecast.ili.LIST)){
+   tmp <- tmp + sim.forecast.ili.LIST[[i]]
+ }
+ 
+ sim.forecast.ili <- tmp/length(sim.forecast.ili.LIST)
+ avg.ili <- sapply(1:length(sim.forecast.ili.LIST), function(k)apply(sim.forecast.ili.LIST[[k]], 2, function(x)sum(x*(0:130))))
+ 
+ data.smooth <- 
+ sapply(1:ncol(sim.forecast.ili), function(k){
+ tmp.mean <- sum(c(0:130)/10*sim.forecast.ili[,k])
+ #tmp.sd <- sd(sample(x=(0:130)/10, 10000, replace=T, prob=sim.forecast.ili[,k]))/2
+ tmp.sd <- sd(sample(x=(0:130)/10, 10000, replace=T, prob=sim.forecast.ili[,k]))/1
+ if(tmp.sd==0)return(sim.forecast.ili[,k])
+ tmp <- dnorm(x=(0:130)/10, mean=tmp.mean, sd=tmp.sd)
+ tmp/sum(tmp)
+ })
+ 
+ 
+ national4wks <- read.csv("national1-4wk.forecast.template.csv")
+ head(national4wks)
+ national4wks$Location <- paste(Region.name[HHS.id])
+ 
+ nrow(national4wks)
+ 
+ national4wks$Value[1] <- signif(sum((0:130)*sim.forecast.ili[,1])/10, 2)
+ national4wks$Value[1+132] <- signif(sum((0:130)*sim.forecast.ili[,2])/10, 2)
+ national4wks$Value[1+132+132] <- signif(sum((0:130)*sim.forecast.ili[,3])/10, 2)
+ national4wks$Value[1+132+132+132] <- signif(sum((0:130)*sim.forecast.ili[,4])/10, 2)
+ 
+ national4wks$Value[(2):(2+130)] <- signif(sim.forecast.ili[,1],2)
+ national4wks$Value[(2+130+2):(2+130+2+130)] <- signif(sim.forecast.ili[,2],2)
+ national4wks$Value[(2+130+2+130+2):(2+130+2+130+2+130)] <- signif(sim.forecast.ili[,3],2)
+ national4wks$Value[(2+130+2+130+2+130+2):(2+130+2+130+2+130+2+130)] <- signif(sim.forecast.ili[,4],2)
+ 
+ write.csv(national4wks, paste("./output/State",HHS.id,"4wks.csv",sep=""), row.names=F)
+ 
+ for(k in 1:ncol(sim.forecast.ili)){
+   sim.forecast.ili[,k] <- data.smooth[,k]
+ }
+ 
+ national4wks <- read.csv("national1-4wk.forecast.template.csv")
+ head(national4wks)
+ national4wks$Location <- paste(Region.name[HHS.id])
+ 
+ nrow(national4wks)
+ 
+ national4wks$Value[1] <- signif(sum((0:130)*sim.forecast.ili[,1])/10, 2)
+ national4wks$Value[1+132] <- signif(sum((0:130)*sim.forecast.ili[,2])/10, 2)
+ national4wks$Value[1+132+132] <- signif(sum((0:130)*sim.forecast.ili[,3])/10, 2)
+ national4wks$Value[1+132+132+132] <- signif(sum((0:130)*sim.forecast.ili[,4])/10, 2)
+ 
+ national4wks$Value[(2):(2+130)] <- signif(sim.forecast.ili[,1],2)
+ national4wks$Value[(2+130+2):(2+130+2+130)] <- signif(sim.forecast.ili[,2],2)
+ national4wks$Value[(2+130+2+130+2):(2+130+2+130+2+130)] <- signif(sim.forecast.ili[,3],2)
+ national4wks$Value[(2+130+2+130+2+130+2):(2+130+2+130+2+130+2+130)] <- signif(sim.forecast.ili[,4],2)
+ 
+ write.csv(national4wks, paste("./output/State",HHS.id,"4wksSMOOTH.csv",sep=""), row.names=F)
+ 
+ 
+ flu.data.wk.prev$ILI[flu.data.wk.prev$YEAR==2018]
+ 
+ projection <- apply(avg.ili,2, function(x)c(flu.data.wk.prev$ILI[flu.data.wk.prev$YEAR==2018], x/10))
+ png(paste("./plots/forecastState",HHS.id,".png",sep=""))
+ plot(week2season(head(wkID,1)-3):week2season(tail(wkID,1)), projection[,1], lty=2,type="l", col="grey", xlab="wk", ylab="ILI%", ylim=c(0.2,3), , main=paste("State: ", Region.name[HHS.id], " 1-4 wk ahead",sep=""))
+ sapply(1:ncol(projection),function(k)lines(week2season(head(wkID,1)-3):week2season(tail(wkID,1)),projection[,k], lty=2,type="l", col="grey"))
+ lines(week2season(head(wkID,1)-3):(week2season(head(wkID,1))-1), flu.data.wk.prev$ILI[flu.data.wk.prev$YEAR==2018], lwd=3)
+ lines((week2season(head(wkID,1))-1):week2season(tail(wkID,1)), c(tail(flu.data.wk.prev$ILI,1), apply(avg.ili, 1, mean)/10), lty=2, col="red",lwd=3)
+ dev.off()
+ 
+ apply(avg.ili, 1, mean)
+ 
+ }
[[1]]
[1] 41.81501 42.28545 42.75746 43.22194

[[2]]
[1] 21.58422 21.82469 22.02067 22.17437

[[3]]
[1] 20.42366 20.55923 20.67797 20.77678

[[4]]
[1] 18.54133 18.72502 18.93606 19.18620

[[5]]
[1] 20.84430 21.29575 21.74092 22.16580

[[6]]
[1] 26.22499 26.33293 26.42278 26.49454

[[7]]
[1] 15.63315 15.76121 15.89202 16.02922

[[8]]
[1] 4.484570 4.608410 4.743796 4.892589

[[9]]
[1] 25.53379 25.63987 25.74684 25.85457

[[10]]
[1] 38.41119 38.67325 38.97163 39.30998

[[11]]
[1] 20.75755 20.80389 20.85126 20.89961

[[12]]
[1] 6.757895 6.893447 7.027509 7.162581

[[13]]
[1] 15.27231 15.63617 16.03038 16.48795

[[14]]
[1] 12.35354 13.13080 14.04545 15.07899

[[15]]
[1] 4.257107 4.423981 4.631683 4.909356

[[16]]
[1] 16.34588 16.49053 16.65482 16.84663

[[17]]
[1] 21.72682 22.03261 22.38801 22.85398

[[18]]
[1] 32.06506 32.20600 32.32675 32.42548

[[19]]
[1] 8.286896 8.324489 8.365010 8.406628

[[20]]
[1] 17.61885 17.70475 17.77979 17.84453

[[21]]
[1] 10.35772 10.41566 10.47829 10.55325

[[22]]
[1] 15.40215 15.73492 16.11098 16.47823

[[23]]
[1] 15.64502 15.56519 15.47996 15.40361

[[24]]
[1] 32.84999 33.53435 34.24050 34.98304

[[25]]
[1] 22.71933 23.14690 23.53829 23.87859

[[26]]
[1] 6.295370 6.299904 6.302170 6.302964

[[27]]
[1] 21.21050 21.22097 21.25195 21.30402

[[28]]
[1] 10.49344 10.64381 10.80237 10.96714

[[29]]
[1] 6.553387 6.552269 6.551195 6.550161

[[30]]
[1] 29.00002 29.00002 29.00002 29.00002

[[31]]
[1] 13.88064 14.04265 14.22416 14.44454

[[32]]
[1] 11.56875 12.14892 12.77325 13.40614

[[33]]
[1] 18.09002 18.86150 19.65896 20.44336

[[34]]
[1] 6.505607 6.526533 6.556246 6.631825

[[35]]
[1] 7.835064 8.017312 8.247137 8.549454

[[36]]
[1] 27.32867 27.87975 28.49022 29.17547

[[37]]
[1] 8.384707 8.434638 8.501082 8.598251

[[38]]
[1] 13.31201 13.64296 14.02451 14.48400

[[39]]
[1] 4.787782 4.816481 4.859367 4.932474

[[40]]
[1] 22.62511 22.75449 22.86470 22.95442

[[41]]
[1] 12.11429 12.27050 12.40430 12.51371

[[42]]
[1] 8.757345 8.938073 9.146310 9.395947

[[43]]
[1] 37.71913 38.14960 38.58197 38.99872

[[44]]
[1] 19.07127 19.07876 19.05991 19.02848

[[45]]
[1]  9.977733 10.208607 10.462908 10.764392

[[46]]
[1] 20.98131 21.08200 21.16736 21.23656

[[47]]
[1] 7.916380 7.973618 8.030325 8.086713

[[48]]
[1]  9.887375  9.960347 10.060045 10.181219

[[49]]
[1] 10.53435 10.55088 10.56802 10.58569

[[50]]
[1] 7.445554 7.578848 7.758882 8.015792

[[51]]
[1] 22.11957 22.32558 22.58063 22.87021

[[52]]
[1] 8.488748 8.515672 8.542563 8.569413

[[53]]
[1] 73.98232 74.19520 74.38234 74.53849

> 
> print(Sys.time()-strt)
Time difference of 56.85811 mins
> stopCluster(cl)
> 
> 
> proc.time()
    user   system  elapsed 
   1.051    0.042 3412.307 
